{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pioro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pioro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pioro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(24094,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "\n",
    "columns=[\"txt\", \"l1-class\", \"l2-class\", \"l3-class\"]\n",
    "train_set = pd.read_csv(\"DBPEDIA_train.csv\", header=None, names=columns)\n",
    "test_set = pd.read_csv(\"DBPEDIA_test.csv\", header=None, names=columns)\n",
    "#Ograniczamy sie do klas l2\n",
    "del train_set[\"l1-class\"]\n",
    "del train_set[\"l3-class\"]\n",
    "del test_set[\"l1-class\"]\n",
    "del test_set[\"l3-class\"]\n",
    "del columns[3]\n",
    "del columns[1]\n",
    "\n",
    "train_set = train_set.sample(frac=0.1)\n",
    "test_set = test_set.sample(frac=0.1)\n",
    "\n",
    "train_set_X = train_set[\"txt\"]\n",
    "train_set_Y = train_set[\"l2-class\"]\n",
    "test_set_X = test_set[\"txt\"]\n",
    "test_set_Y = test_set[\"l2-class\"]\n",
    "\n",
    "print(type(train_set_X))\n",
    "print(train_set_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the darmstadt–worms railway is a standardgauge railway that is now partially closed it runs through southern hesse through the hessian ried hessische ried and so it is also called the riedbahn ried railway the section between darmstadt and riedstadtgoddelau is now largely closed the section between riedstadtgoddelau and biblis which is now considered part of the mannheim–frankfurt railway is of great importance for longdistance passenger services and rail freight traffic the last section from biblis to worms is used by regional passenger services and rail freight traffic\n"
     ]
    }
   ],
   "source": [
    "train_set_X = train_set_X.apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n",
    "test_set_X = test_set_X.apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "train_set_X = train_set_X.apply(lambda text: text.lower())\n",
    "test_set_X = test_set_X.apply(lambda text: text.lower())\n",
    "\n",
    "print(train_set_X.iloc[4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'darmstadt–worms', 'railway', 'is', 'a', 'standardgauge', 'railway', 'that', 'is', 'now', 'partially', 'closed', 'it', 'runs', 'through', 'southern', 'hesse', 'through', 'the', 'hessian', 'ried', 'hessische', 'ried', 'and', 'so', 'it', 'is', 'also', 'called', 'the', 'riedbahn', 'ried', 'railway', 'the', 'section', 'between', 'darmstadt', 'and', 'riedstadtgoddelau', 'is', 'now', 'largely', 'closed', 'the', 'section', 'between', 'riedstadtgoddelau', 'and', 'biblis', 'which', 'is', 'now', 'considered', 'part', 'of', 'the', 'mannheim–frankfurt', 'railway', 'is', 'of', 'great', 'importance', 'for', 'longdistance', 'passenger', 'services', 'and', 'rail', 'freight', 'traffic', 'the', 'last', 'section', 'from', 'biblis', 'to', 'worms', 'is', 'used', 'by', 'regional', 'passenger', 'services', 'and', 'rail', 'freight', 'traffic']\n"
     ]
    }
   ],
   "source": [
    "train_set_X = train_set_X.apply(lambda text: word_tokenize(text))\n",
    "test_set_X = test_set_X.apply(lambda text: word_tokenize(text))\n",
    "\n",
    "print(train_set_X.iloc[4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "['the', 'darmstadt–worms', 'railway', 'be', 'a', 'standardgauge', 'railway', 'that', 'be', 'now', 'partially', 'close', 'it', 'run', 'through', 'southern', 'hesse', 'through', 'the', 'hessian', 'ried', 'hessische', 'ried', 'and', 'so', 'it', 'be', 'also', 'call', 'the', 'riedbahn', 'ried', 'railway', 'the', 'section', 'between', 'darmstadt', 'and', 'riedstadtgoddelau', 'be', 'now', 'largely', 'close', 'the', 'section', 'between', 'riedstadtgoddelau', 'and', 'biblis', 'which', 'be', 'now', 'consider', 'part', 'of', 'the', 'mannheim–frankfurt', 'railway', 'be', 'of', 'great', 'importance', 'for', 'longdistance', 'passenger', 'service', 'and', 'rail', 'freight', 'traffic', 'the', 'last', 'section', 'from', 'biblis', 'to', 'worm', 'be', 'use', 'by', 'regional', 'passenger', 'service', 'and', 'rail', 'freight', 'traffic']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "iterator = 0\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def list_lemmatize(list_of_words):\n",
    "    new_list_of_words = []\n",
    "    global iterator\n",
    "    iterator += 1\n",
    "    if iterator%10000 == 0:\n",
    "        print(iterator)\n",
    "    for word in list_of_words:\n",
    "        new_list_of_words.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "    return new_list_of_words\n",
    "\n",
    "train_set_X = train_set_X.apply(list_lemmatize)\n",
    "test_set_X = test_set_X.apply(list_lemmatize)\n",
    "\n",
    "print(train_set_X.iloc[4])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pioro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darmstadt–worms', 'railway', 'standardgauge', 'railway', 'partially', 'close', 'run', 'southern', 'hesse', 'hessian', 'ried', 'hessische', 'ried', 'also', 'call', 'riedbahn', 'ried', 'railway', 'section', 'darmstadt', 'riedstadtgoddelau', 'largely', 'close', 'section', 'riedstadtgoddelau', 'biblis', 'consider', 'part', 'mannheim–frankfurt', 'railway', 'great', 'importance', 'longdistance', 'passenger', 'service', 'rail', 'freight', 'traffic', 'last', 'section', 'biblis', 'worm', 'use', 'regional', 'passenger', 'service', 'rail', 'freight', 'traffic']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stop_words(list_of_words):\n",
    "    words_filtered = []\n",
    "    for w in list_of_words:\n",
    "        if w not in stopWords:\n",
    "            words_filtered.append(w)\n",
    "    return words_filtered\n",
    "\n",
    "train_set_X = train_set_X.apply(filter_stop_words)\n",
    "test_set_X = test_set_X.apply(filter_stop_words)\n",
    "\n",
    "print(train_set_X.iloc[4])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darmstadt–worms railway standardgauge railway partially close run southern hesse hessian ried hessische ried also call riedbahn ried railway section darmstadt riedstadtgoddelau largely close section riedstadtgoddelau biblis consider part mannheim–frankfurt railway great importance longdistance passenger service rail freight traffic last section biblis worm use regional passenger service rail freight traffic\n"
     ]
    }
   ],
   "source": [
    "train_set_X = train_set_X.apply(lambda text: ' '.join(text))\n",
    "test_set_X = test_set_X.apply(lambda text: ' '.join(text))\n",
    "\n",
    "print(train_set_X.iloc[4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24094, 17250)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.5)\n",
    "train_features_set_X = vectorizer.fit_transform(train_set_X)\n",
    "test_features_set_X = vectorizer.transform(test_set_X)\n",
    "print(train_features_set_X.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336764339669628\n",
      "0.8657894736842106\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bayes = MultinomialNB()\n",
    "bayes.fit(train_features_set_X, train_set_Y)\n",
    "print(bayes.score(train_features_set_X, train_set_Y))\n",
    "print(bayes.score(test_features_set_X, test_set_Y))\n",
    "print(len(set(test_set_Y)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tournament[ True]\n",
      "fa cup 1973–74 93rd season world old football knockout competition football association challenge cup fa cup short large number club enter tournament low english football league system meant competition start number preliminary qualify round 30 victorious team fourth round qualify progress first round proper\n"
     ]
    }
   ],
   "source": [
    "index = 88\n",
    "prediction = bayes.predict(test_features_set_X[index])\n",
    "print(prediction[0] + str(test_set_Y.iloc[index] == prediction))\n",
    "print(test_set_X.iloc[index])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}